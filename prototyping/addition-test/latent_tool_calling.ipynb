{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "564f1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54a793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import itertools\n",
    "\n",
    "class AddTokenizer:\n",
    "    def __init__(self, min_int=-500, max_int=500):\n",
    "        self.min_int = min_int\n",
    "        self.max_int = max_int\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = []\n",
    "        self.add_special_tokens()\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def add_special_tokens(self):\n",
    "        specials = [\"+\", \"-\", \"*\", \"/\", \"=\"]\n",
    "        for tok in specials:\n",
    "            self.token_to_id[tok] = len(self.id_to_token)\n",
    "            self.id_to_token.append(tok)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        for i in range(self.min_int, self.max_int + 1):\n",
    "            tok = str(i)\n",
    "            self.token_to_id[tok] = len(self.id_to_token)\n",
    "            self.id_to_token.append(tok)\n",
    "\n",
    "    def encode_int(self, i):\n",
    "        return [self.token_to_id[str(i)]]\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.token_to_id[s]]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.id_to_token[i] for i in ids)\n",
    "\n",
    "\n",
    "def build_addition_dataset(min_int=-500, max_int=500):\n",
    "    tokenizer = AddTokenizer(min_int, max_int)\n",
    "\n",
    "    X = []\n",
    "    mask = []\n",
    "\n",
    "    for a, b in itertools.product(range(min_int, max_int + 1),\n",
    "                                  range(min_int, max_int + 1)):\n",
    "\n",
    "        c = a + b\n",
    "        if not (min_int <= c <= max_int):\n",
    "            continue\n",
    "\n",
    "        seq = (\n",
    "            tokenizer.encode_int(a)\n",
    "            + tokenizer.encode(\"+\")\n",
    "            + tokenizer.encode_int(b)\n",
    "            + tokenizer.encode(\"=\")\n",
    "            + tokenizer.encode_int(c)\n",
    "        )\n",
    "\n",
    "        # convert to tensor\n",
    "        X.append(seq)\n",
    "        mask.append([1] * len(seq))\n",
    "\n",
    "    # convert lists to tensors of shape [N, L]\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "    return tokenizer, X, mask\n",
    "\n",
    "\n",
    "def train_val_split(X, mask, val_frac=0.1):\n",
    "    N = len(X)\n",
    "    val_size = int(N * val_frac)\n",
    "    train_size = N - val_size\n",
    "    train_data, val_data = random_split(\n",
    "        TensorDataset(X, mask),\n",
    "        [train_size, val_size]\n",
    "    )\n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def make_addition_dataset(max_int=500):\n",
    "    # all ordered pairs (a,b)\n",
    "    a_vals = torch.arange(0, max_int+1)\n",
    "    b_vals = torch.arange(0, max_int+1)\n",
    "    A, B = torch.meshgrid(a_vals, b_vals, indexing='ij')\n",
    "\n",
    "    A = A.reshape(-1)      # (N,)\n",
    "    B = B.reshape(-1)      # (N,)\n",
    "    C = A + B              # (N,)\n",
    "\n",
    "    # filter valid sums\n",
    "    valid = (C <= max_int)\n",
    "    A = A[valid]\n",
    "    B = B[valid]\n",
    "    C = C[valid]\n",
    "\n",
    "    X = torch.stack([A, B, C], dim=1)  # (N, 3)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def train_val_split(X, val_frac=0.1):\n",
    "    N = len(X)\n",
    "    val_size = int(N * val_frac)\n",
    "    train_size = N - val_size\n",
    "    ds = TensorDataset(X)\n",
    "    return random_split(ds, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "861f449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125751, 3])\n",
      "(tensor([ 23, 119, 142]),)\n"
     ]
    }
   ],
   "source": [
    "X = make_addition_dataset(max_int=500)\n",
    "train_data, val_data = train_val_split(X)\n",
    "\n",
    "print(X.shape)      # → [~251k examples, 3]\n",
    "print(train_data[0]) # → tensor([a, b, a+b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f002ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankExpert(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(rank, out_dim, bias=False)\n",
    "        self.B = nn.Linear(in_dim, rank, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.A(self.B(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5ff6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEWithLoadBalancing(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_experts, dropout=0.0, top_k=1):\n",
    "        super().__init__()\n",
    "        if top_k != 1:\n",
    "            raise NotImplementedError(\"Top-k is not implemented for MoE\")\n",
    "        self.top_k = top_k\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # self.experts = nn.ModuleList([nn.Linear(in_dim, out_dim) for _ in range(num_experts)])\n",
    "        self.experts = nn.ModuleList([LowRankExpert(in_dim, out_dim, 4) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x, expert_probs, return_load_balance_loss=False):\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        if expert_probs.shape != (B, self.num_experts):\n",
    "            raise ValueError(f\"Expert probabilities must be of shape (B, num_experts), got {expert_probs.shape}\")\n",
    "        \n",
    "        expert_idx = torch.argmax(expert_probs, dim=-1)\n",
    "\n",
    "        x_out = torch.zeros(B, S, self.out_dim, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = expert_idx == i\n",
    "            if mask.any():\n",
    "                x_out[mask, :, :] = expert(x[mask, :, :])\n",
    "\n",
    "        if return_load_balance_loss:\n",
    "            lb_loss = compute_load_balancing_loss(expert_probs, expert_idx, self.num_experts)\n",
    "            return x_out, expert_idx, lb_loss\n",
    "        \n",
    "        return x_out, expert_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a0acd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockWithLoadBalancing(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, dropout=0.0, is_causal=True, \n",
    "                 use_moe=False, num_experts=4, router_idx=None, verbose_router=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_experts = num_experts\n",
    "        self.dropout = dropout\n",
    "        self.router_idx = router_idx\n",
    "        self.is_causal = is_causal\n",
    "        self.verbose_router = verbose_router\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.use_moe = use_moe\n",
    "        if use_moe:\n",
    "            if router_idx is None:\n",
    "                raise ValueError(\"router_idx must be provided when using MoE\")\n",
    "            self.router = nn.Linear(dim, num_experts)\n",
    "            self.qkv = MoEWithLoadBalancing(dim, 3 * dim, num_experts, dropout=dropout)\n",
    "            self.o = MoEWithLoadBalancing(dim, dim, num_experts, dropout=dropout)\n",
    "            self.mlp_in = MoEWithLoadBalancing(dim, int(dim * mlp_ratio), num_experts, dropout=dropout)\n",
    "            self.mlp_out = MoEWithLoadBalancing(int(dim * mlp_ratio), dim, num_experts, dropout=dropout)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, 3 * dim)\n",
    "            self.o = nn.Linear(dim, dim)\n",
    "            self.mlp_in = nn.Linear(dim, int(dim * mlp_ratio))\n",
    "            self.mlp_out = nn.Linear(int(dim * mlp_ratio), dim)\n",
    "\n",
    "    def forward(self, x, return_load_balance_loss=False):\n",
    "        B, S, D = x.shape\n",
    "        total_lb_loss = 0.0\n",
    "        \n",
    "        # Attention block\n",
    "        if self.use_moe:\n",
    "            router_out = self.router(x[:, self.router_idx])\n",
    "            expert_probs = F.softmax(router_out, dim=-1)\n",
    "            \n",
    "            if self.verbose_router:\n",
    "                top_experts = torch.argmax(expert_probs, dim=-1)\n",
    "                counts = torch.bincount(top_experts.flatten(), minlength=self.num_experts)\n",
    "                usage = counts.float() / counts.sum() * 100\n",
    "                print(f\"Expert usage (%): {[f'{u:.1f}' for u in usage.tolist()]}\")\n",
    "            \n",
    "            # QKV projection\n",
    "            if return_load_balance_loss:\n",
    "                qkv, _, lb_loss_qkv = self.qkv(x, expert_probs, return_load_balance_loss=True)\n",
    "                total_lb_loss += lb_loss_qkv\n",
    "            else:\n",
    "                qkv, _ = self.qkv(x, expert_probs)\n",
    "        else:\n",
    "            qkv = self.qkv(x)\n",
    "\n",
    "        # Split into Q, K, V and compute attention\n",
    "        q, k, v = qkv.split(D, dim=2)\n",
    "        q = q.view(B, S, self.n_heads, D // self.n_heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, S, self.n_heads, D // self.n_heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, S, self.n_heads, D // self.n_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=self.is_causal)\n",
    "        attn_out = attn_out.permute(0, 2, 1, 3).contiguous().view(B, S, D)\n",
    "        \n",
    "        # Output projection\n",
    "        if self.use_moe:\n",
    "            if return_load_balance_loss:\n",
    "                attn_out, _, lb_loss_o = self.o(attn_out, expert_probs, return_load_balance_loss=True)\n",
    "                total_lb_loss += lb_loss_o\n",
    "            else:\n",
    "                attn_out, _ = self.o(attn_out, expert_probs)\n",
    "        else:\n",
    "            attn_out = self.o(attn_out)\n",
    "        \n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # MLP block\n",
    "        if self.use_moe:\n",
    "            if return_load_balance_loss:\n",
    "                mlp_out, _, lb_loss_mlp_in = self.mlp_in(x, expert_probs, return_load_balance_loss=True)\n",
    "                total_lb_loss += lb_loss_mlp_in\n",
    "            else:\n",
    "                mlp_out, _ = self.mlp_in(x, expert_probs)\n",
    "            \n",
    "            mlp_out = F.gelu(mlp_out)\n",
    "            \n",
    "            if return_load_balance_loss:\n",
    "                mlp_out, _, lb_loss_mlp_out = self.mlp_out(mlp_out, expert_probs, return_load_balance_loss=True)\n",
    "                total_lb_loss += lb_loss_mlp_out\n",
    "            else:\n",
    "                mlp_out, _ = self.mlp_out(mlp_out, expert_probs)\n",
    "        else:\n",
    "            mlp_out = F.gelu(self.mlp_in(x))\n",
    "            mlp_out = self.mlp_out(mlp_out)\n",
    "        \n",
    "        x = self.norm2(x + mlp_out)\n",
    "        \n",
    "        if return_load_balance_loss:\n",
    "            return x, total_lb_loss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0c231e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "D_MODEL = 32\n",
    "N_HEADS = 2\n",
    "DEPTH = 4\n",
    "LOAD_BALANCE_COEFF = 0.01  # Alpha parameter for load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c7ef3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, depth):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerBlockWithLoadBalancing(d_model, n_heads, mlp_ratio=4, is_causal=False, use_moe=False)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.project = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # Embed the input first: [B, 3] -> [B, 3, D]\n",
    "        for tf in self.transformers:\n",
    "            x = tf(x)\n",
    "\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "        # # embedding → scalar value\n",
    "        # self.to_num = nn.Linear(d_model, 1)\n",
    "\n",
    "        # # numeric result → embedding update\n",
    "        # self.from_num = nn.Linear(1, d_model)\n",
    "\n",
    "        # # gate\n",
    "        # self.gate = nn.Linear(2 * d_model, 1)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # x shape: [batch, 3]  (n1, n2, target_sum)\n",
    "\n",
    "    #     emb = self.embed(x)           # [B, 3, D]\n",
    "    #     h1, h2 = emb[:, 0], emb[:, 1]\n",
    "\n",
    "    #     num1 = self.to_num(h1)        # [B, 1]\n",
    "    #     num2 = self.to_num(h2)        # [B, 1]\n",
    "\n",
    "    #     num_sum = num1 + num2         # pure operation\n",
    "\n",
    "    #     # update latent\n",
    "    #     gate = torch.sigmoid(self.gate(torch.cat([h1, h2], dim=-1)))  # [B, 1]\n",
    "    #     update = self.from_num(num_sum)\n",
    "\n",
    "    #     # apply update to h1 only (simple test)\n",
    "    #     h1_new = h1 + gate * update\n",
    "\n",
    "    #     return num_sum, num1, num2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81270b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "dataset = AdditionDataset(X)\n",
    "loader = DataLoader(dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3589e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = X.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5f266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f09dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.367236614227295\n",
      "6.358667373657227\n",
      "6.307708263397217\n",
      "6.281274795532227\n",
      "6.192593097686768\n",
      "6.186535358428955\n",
      "6.059232234954834\n",
      "6.0523762702941895\n",
      "5.9908270835876465\n",
      "5.987705230712891\n",
      "5.975044250488281\n",
      "5.910215854644775\n",
      "5.892751693725586\n",
      "5.812833786010742\n",
      "5.80508279800415\n",
      "5.766063690185547\n",
      "5.6840386390686035\n",
      "5.698946475982666\n",
      "5.684512615203857\n",
      "5.637073993682861\n",
      "5.6229023933410645\n",
      "5.541421890258789\n",
      "5.505592346191406\n",
      "5.516163349151611\n",
      "5.46511697769165\n",
      "5.442641735076904\n",
      "5.413387298583984\n",
      "5.356739521026611\n",
      "5.357078552246094\n",
      "5.336609363555908\n",
      "5.25968599319458\n",
      "5.232937335968018\n",
      "5.163944721221924\n",
      "5.152922630310059\n",
      "5.168474197387695\n",
      "5.141955852508545\n",
      "5.095445156097412\n",
      "5.066081523895264\n",
      "5.0337958335876465\n",
      "5.0008440017700195\n",
      "4.962721347808838\n",
      "4.9335103034973145\n",
      "4.895631313323975\n",
      "4.915548801422119\n",
      "4.872579097747803\n",
      "4.81920051574707\n",
      "4.798313617706299\n",
      "4.78100061416626\n",
      "4.7504777908325195\n",
      "4.695327281951904\n",
      "4.672508239746094\n",
      "4.654851913452148\n",
      "4.592723369598389\n",
      "4.575587749481201\n",
      "4.575484752655029\n",
      "4.557520389556885\n",
      "4.46531343460083\n",
      "4.444918155670166\n",
      "4.396655082702637\n",
      "4.4138054847717285\n",
      "4.3714189529418945\n",
      "4.3513407707214355\n",
      "4.290157794952393\n",
      "4.250799655914307\n",
      "4.233677387237549\n",
      "4.215147495269775\n",
      "4.175752639770508\n",
      "4.133600234985352\n",
      "4.128103733062744\n",
      "4.06133508682251\n",
      "3.9615976810455322\n",
      "4.009552478790283\n",
      "3.980952501296997\n",
      "3.9729955196380615\n",
      "3.8956305980682373\n",
      "3.8621985912323\n",
      "3.86458683013916\n",
      "3.8166534900665283\n",
      "3.78851056098938\n",
      "3.7417705059051514\n",
      "3.7279834747314453\n",
      "3.6874592304229736\n",
      "3.6770429611206055\n",
      "3.680753707885742\n",
      "3.602055311203003\n",
      "3.549947738647461\n",
      "3.5309813022613525\n",
      "3.5090954303741455\n",
      "3.4746780395507812\n",
      "3.4138548374176025\n",
      "3.4300715923309326\n",
      "3.3809120655059814\n",
      "3.3429300785064697\n",
      "3.3102681636810303\n",
      "3.2859153747558594\n",
      "3.2555246353149414\n",
      "3.2011911869049072\n",
      "3.208662748336792\n",
      "3.119774580001831\n",
      "3.1038124561309814\n",
      "3.065232992172241\n",
      "3.0546014308929443\n",
      "3.024855852127075\n",
      "2.997941017150879\n",
      "2.951420545578003\n",
      "2.944631338119507\n",
      "2.8837127685546875\n",
      "2.877671957015991\n",
      "2.838186025619507\n",
      "2.8306853771209717\n",
      "2.737384796142578\n",
      "2.7644894123077393\n",
      "2.756279945373535\n",
      "2.7313039302825928\n",
      "2.693885087966919\n",
      "2.6487925052642822\n",
      "2.6010258197784424\n",
      "2.6068880558013916\n",
      "2.5777831077575684\n",
      "2.5547070503234863\n",
      "2.4560768604278564\n",
      "2.437952756881714\n",
      "2.4234516620635986\n",
      "2.422745704650879\n",
      "2.376957654953003\n",
      "2.3383543491363525\n",
      "2.3543026447296143\n",
      "2.320178270339966\n",
      "2.290663003921509\n",
      "2.274531126022339\n",
      "2.1931099891662598\n",
      "2.1839921474456787\n",
      "2.1989386081695557\n",
      "2.1015663146972656\n",
      "2.1188673973083496\n",
      "2.0722100734710693\n",
      "2.032489061355591\n",
      "2.0221314430236816\n",
      "2.015866994857788\n",
      "1.9746955633163452\n",
      "1.9249200820922852\n",
      "1.9261671304702759\n",
      "1.877576470375061\n",
      "1.876126766204834\n",
      "1.8483080863952637\n",
      "1.8663960695266724\n",
      "1.7816320657730103\n",
      "1.7611736059188843\n",
      "1.7605994939804077\n",
      "1.7230061292648315\n",
      "1.6876235008239746\n",
      "1.689515471458435\n",
      "1.6457839012145996\n",
      "1.6350431442260742\n",
      "1.6070321798324585\n",
      "1.6112842559814453\n",
      "1.591442584991455\n",
      "1.5026048421859741\n",
      "1.5310431718826294\n",
      "1.5123146772384644\n",
      "1.4862008094787598\n",
      "1.4956709146499634\n",
      "1.4315409660339355\n",
      "1.4104079008102417\n",
      "1.4036670923233032\n",
      "1.3709019422531128\n",
      "1.3539999723434448\n",
      "1.3302862644195557\n",
      "1.3287320137023926\n",
      "1.319045901298523\n",
      "1.2825580835342407\n",
      "1.2698936462402344\n",
      "1.245619297027588\n",
      "1.2388638257980347\n",
      "1.2188175916671753\n",
      "1.1960054636001587\n",
      "1.1731311082839966\n",
      "1.1483969688415527\n",
      "1.1432507038116455\n",
      "1.09951651096344\n",
      "1.0948206186294556\n",
      "1.063838243484497\n",
      "1.0867846012115479\n",
      "1.0730817317962646\n",
      "1.008545994758606\n",
      "1.0167908668518066\n",
      "1.008103370666504\n",
      "0.9662129878997803\n",
      "0.9781088829040527\n",
      "0.9604049324989319\n",
      "0.9451178908348083\n",
      "0.9561485648155212\n",
      "0.9238529801368713\n",
      "0.8795561790466309\n",
      "0.9015250205993652\n",
      "0.8767390847206116\n",
      "0.8747272491455078\n",
      "0.8539044260978699\n",
      "0.8411696553230286\n",
      "0.8375146985054016\n",
      "0.8147299885749817\n",
      "0.7953718304634094\n",
      "0.7979156970977783\n",
      "0.7862842679023743\n",
      "0.7712551951408386\n",
      "0.7444277405738831\n",
      "0.754919707775116\n",
      "0.7253233790397644\n",
      "0.7334617972373962\n",
      "0.7109313607215881\n",
      "0.7138640880584717\n",
      "0.6820688247680664\n",
      "0.6723194718360901\n",
      "0.6778860092163086\n",
      "0.6602371335029602\n",
      "0.6644188761711121\n",
      "0.6300651431083679\n",
      "0.6373782753944397\n",
      "0.6191542744636536\n",
      "0.6188206076622009\n",
      "0.6049734950065613\n",
      "0.5866051912307739\n",
      "0.577883780002594\n",
      "0.569316565990448\n",
      "0.56770920753479\n",
      "0.5542622804641724\n",
      "0.5557342171669006\n",
      "0.5513167977333069\n",
      "0.532734215259552\n",
      "0.528449296951294\n",
      "0.5207117795944214\n",
      "0.5173370242118835\n",
      "0.5034870505332947\n",
      "0.5080087780952454\n",
      "0.49973249435424805\n",
      "0.47907352447509766\n",
      "0.47745415568351746\n",
      "0.4908672571182251\n",
      "0.4609949588775635\n",
      "0.46837055683135986\n",
      "0.4569324553012848\n",
      "0.45716986060142517\n",
      "0.44196656346321106\n",
      "0.4326956570148468\n",
      "0.43498238921165466\n",
      "0.41679373383522034\n",
      "0.42424604296684265\n",
      "0.4213959872722626\n",
      "0.416526198387146\n",
      "0.3959786593914032\n",
      "0.4072091579437256\n",
      "0.38571199774742126\n",
      "0.388338565826416\n",
      "0.3842215836048126\n",
      "0.3881709575653076\n",
      "0.37806954979896545\n",
      "0.3687390983104706\n",
      "0.36927488446235657\n",
      "0.36463919281959534\n",
      "0.3566809594631195\n",
      "0.352133184671402\n",
      "0.3527836799621582\n",
      "0.3415035903453827\n",
      "0.33829471468925476\n",
      "0.3318854868412018\n",
      "0.32963481545448303\n",
      "0.32711514830589294\n",
      "0.32288703322410583\n",
      "0.314135879278183\n",
      "0.3154156506061554\n",
      "0.30649179220199585\n",
      "0.31060853600502014\n",
      "0.3079646825790405\n",
      "0.30249109864234924\n",
      "0.2944459617137909\n",
      "0.29556575417518616\n",
      "0.2952464520931244\n",
      "0.2927955389022827\n",
      "0.28662195801734924\n",
      "0.28372058272361755\n",
      "0.2792111933231354\n",
      "0.2748836576938629\n",
      "0.2759071886539459\n",
      "0.270337849855423\n",
      "0.26884105801582336\n",
      "0.2635836899280548\n",
      "0.2595311105251312\n",
      "0.2579846680164337\n",
      "0.25611236691474915\n",
      "0.24830830097198486\n",
      "0.24700625240802765\n",
      "0.24300573766231537\n",
      "0.24530941247940063\n",
      "0.24482212960720062\n",
      "0.2411407232284546\n",
      "0.23959018290042877\n",
      "0.2380005568265915\n",
      "0.23673009872436523\n",
      "0.23063619434833527\n",
      "0.2262437492609024\n",
      "0.2252787947654724\n",
      "0.22659124433994293\n",
      "0.2199152708053589\n",
      "0.22008831799030304\n",
      "0.22023023664951324\n",
      "0.21795958280563354\n",
      "0.21190650761127472\n",
      "0.21103256940841675\n",
      "0.21010822057724\n",
      "0.2073197364807129\n",
      "0.2055988907814026\n",
      "0.20150817930698395\n",
      "0.1983381062746048\n",
      "0.19978536665439606\n",
      "0.19873327016830444\n",
      "0.19556783139705658\n",
      "0.19363819062709808\n",
      "0.19388192892074585\n",
      "0.19248066842556\n",
      "0.18659110367298126\n",
      "0.18823324143886566\n",
      "0.18489272892475128\n",
      "0.18640513718128204\n",
      "0.1829279214143753\n",
      "0.1780569702386856\n",
      "0.1777740865945816\n",
      "0.17475683987140656\n",
      "0.1753522902727127\n",
      "0.17400304973125458\n",
      "0.1744781732559204\n",
      "0.1697155237197876\n",
      "0.17053140699863434\n",
      "0.16949611902236938\n",
      "0.16741351783275604\n",
      "0.1674378663301468\n",
      "0.16469688713550568\n",
      "0.1645527333021164\n",
      "0.16234201192855835\n",
      "0.15975874662399292\n",
      "0.16037066280841827\n",
      "0.15782128274440765\n",
      "0.15583227574825287\n",
      "0.15727560222148895\n",
      "0.15441392362117767\n",
      "0.15369874238967896\n",
      "0.15276478230953217\n",
      "0.14994630217552185\n",
      "0.15058369934558868\n",
      "0.14648516476154327\n",
      "0.14875097572803497\n",
      "0.1475110501050949\n",
      "0.14486655592918396\n",
      "0.14393684267997742\n",
      "0.14164379239082336\n",
      "0.14254429936408997\n",
      "0.1423320323228836\n",
      "0.14082418382167816\n",
      "0.1392204314470291\n",
      "0.13861408829689026\n",
      "0.13779719173908234\n",
      "0.13543285429477692\n",
      "0.1357448846101761\n",
      "0.13464295864105225\n",
      "0.13329458236694336\n",
      "0.13298030197620392\n",
      "0.13293005526065826\n",
      "0.12820158898830414\n",
      "0.12858498096466064\n",
      "0.12898652255535126\n",
      "0.12770842015743256\n",
      "0.12803921103477478\n",
      "0.1258937269449234\n",
      "0.12610921263694763\n",
      "0.1251261830329895\n",
      "0.12312892079353333\n",
      "0.12388112396001816\n",
      "0.12245139479637146\n",
      "0.12196895480155945\n",
      "0.12250318378210068\n",
      "0.12028887867927551\n",
      "0.11752385646104813\n",
      "0.11596384644508362\n",
      "0.11899887770414352\n",
      "0.11612159758806229\n",
      "0.11647260189056396\n",
      "0.11614307761192322\n",
      "0.11594725400209427\n",
      "0.11273825168609619\n",
      "0.11147855967283249\n",
      "0.11130928993225098\n",
      "0.11168679594993591\n",
      "0.11032161861658096\n",
      "0.11057379841804504\n",
      "0.11107867956161499\n",
      "0.10642367601394653\n",
      "0.10771063715219498\n",
      "0.10587546229362488\n",
      "0.10606720298528671\n",
      "0.10478917509317398\n",
      "0.10596131533384323\n",
      "0.1045665517449379\n",
      "0.10385730862617493\n",
      "0.10299073904752731\n",
      "0.10176707059144974\n",
      "0.10225602239370346\n",
      "0.10228580981492996\n",
      "0.10072536021471024\n",
      "0.1008547767996788\n",
      "0.09880562871694565\n",
      "0.09992345422506332\n",
      "0.0978909507393837\n",
      "0.09801460057497025\n",
      "0.09652909636497498\n",
      "0.09608229249715805\n",
      "0.0945449247956276\n",
      "0.09533164650201797\n",
      "0.09484704583883286\n",
      "0.09513810276985168\n",
      "0.09202295541763306\n",
      "0.09290961176156998\n",
      "0.09279555827379227\n",
      "0.09236734360456467\n",
      "0.09176743775606155\n",
      "0.08988577127456665\n",
      "0.09078747779130936\n",
      "0.09027921408414841\n",
      "0.0884103775024414\n",
      "0.08914972096681595\n",
      "0.08893335610628128\n",
      "0.08774629980325699\n",
      "0.087851382791996\n",
      "0.08645319193601608\n",
      "0.08679288625717163\n",
      "0.08579224348068237\n",
      "0.08645649999380112\n",
      "0.08521252870559692\n",
      "0.08492595702409744\n",
      "0.08389080315828323\n",
      "0.08449658751487732\n",
      "0.08260893076658249\n",
      "0.08289600908756256\n",
      "0.08203447610139847\n",
      "0.0821145549416542\n",
      "0.08160237222909927\n",
      "0.08173616975545883\n",
      "0.08132640272378922\n",
      "0.0805167481303215\n",
      "0.07890769839286804\n",
      "0.08006296306848526\n",
      "0.0796307772397995\n",
      "0.0786309465765953\n",
      "0.07827761024236679\n",
      "0.07800795882940292\n",
      "0.07740610092878342\n",
      "0.07761809974908829\n",
      "0.07682835310697556\n",
      "0.0764501765370369\n",
      "0.0761464312672615\n",
      "0.07601924985647202\n",
      "0.07409576326608658\n",
      "0.07496138662099838\n",
      "0.07497505843639374\n",
      "0.07456941902637482\n",
      "0.07319991290569305\n",
      "0.07311083376407623\n",
      "0.07252441346645355\n",
      "0.07217676937580109\n",
      "0.07219667732715607\n",
      "0.07086484879255295\n",
      "0.07130537182092667\n",
      "0.07111731916666031\n",
      "0.07020298391580582\n",
      "0.07033912092447281\n",
      "0.07100016623735428\n",
      "0.07014954090118408\n",
      "0.07050508260726929\n",
      "0.06863611191511154\n",
      "0.0694076269865036\n",
      "0.06774801015853882\n",
      "0.06876426935195923\n",
      "0.0671963319182396\n",
      "0.06726760417222977\n",
      "0.06711454689502716\n",
      "0.06693423539400101\n",
      "0.06634452939033508\n",
      "0.06654136627912521\n",
      "0.06576506048440933\n",
      "0.06470606476068497\n",
      "0.0655057430267334\n",
      "0.06516800820827484\n",
      "0.06473585218191147\n",
      "0.06395156681537628\n",
      "[step 0] loss=0.0640  \n",
      "0.06422343850135803\n",
      "0.06430602818727493\n",
      "0.06287983804941177\n",
      "0.06248347461223602\n",
      "0.06266999244689941\n",
      "0.06174403429031372\n",
      "0.06148798391222954\n",
      "0.061754848808050156\n",
      "0.06086059287190437\n",
      "0.06149010360240936\n",
      "0.061485305428504944\n",
      "0.061098113656044006\n",
      "0.06047680601477623\n",
      "0.060021743178367615\n",
      "0.06004155054688454\n",
      "0.059097494930028915\n",
      "0.059821244329214096\n",
      "0.05896986648440361\n",
      "0.05957207456231117\n",
      "0.058617010712623596\n",
      "0.05896653234958649\n",
      "0.058853477239608765\n",
      "0.05844620242714882\n",
      "0.0578315369784832\n",
      "0.05888945236802101\n",
      "0.05753748118877411\n",
      "0.057234685868024826\n",
      "0.05670657381415367\n",
      "0.05612647533416748\n",
      "0.05596369877457619\n",
      "0.05579834058880806\n",
      "0.05636872723698616\n",
      "0.0550357885658741\n",
      "0.05591215565800667\n",
      "0.05489649251103401\n",
      "0.054826658219099045\n",
      "0.05473189428448677\n",
      "0.05421954020857811\n",
      "0.05479024350643158\n",
      "0.05389784276485443\n",
      "0.05368071421980858\n",
      "0.05390334129333496\n",
      "0.05339367315173149\n",
      "0.05241973325610161\n",
      "0.0524374358355999\n",
      "0.05251935124397278\n",
      "0.052718132734298706\n",
      "0.05229611322283745\n",
      "0.051809635013341904\n",
      "0.05173392593860626\n",
      "0.051663219928741455\n",
      "0.051844701170921326\n",
      "0.05114637687802315\n",
      "0.05182512477040291\n",
      "0.05116353556513786\n",
      "0.05118466913700104\n",
      "0.05025742948055267\n",
      "0.05017116665840149\n",
      "0.050520747900009155\n",
      "0.04973924160003662\n",
      "0.04952940717339516\n",
      "0.04927395284175873\n",
      "0.04908568039536476\n",
      "0.04932356998324394\n",
      "0.048237159848213196\n",
      "0.04870062693953514\n",
      "0.048045527189970016\n",
      "0.048328425735235214\n",
      "0.04837758466601372\n",
      "0.0480799525976181\n",
      "0.04773623123764992\n",
      "0.04708230495452881\n",
      "0.04695957899093628\n",
      "0.048040445894002914\n",
      "0.04775518551468849\n",
      "0.04649806022644043\n",
      "0.04722885414958\n",
      "0.046690601855516434\n",
      "0.04578922688961029\n",
      "0.04680115357041359\n",
      "0.04599272832274437\n",
      "0.0453931950032711\n",
      "0.045910436660051346\n",
      "0.04540616646409035\n",
      "0.045613184571266174\n",
      "0.045128580182790756\n",
      "0.04552711173892021\n",
      "0.04529288038611412\n",
      "0.045018017292022705\n",
      "0.04491421580314636\n",
      "0.04388538375496864\n",
      "0.044187188148498535\n",
      "0.0439213328063488\n",
      "0.043358366936445236\n",
      "0.043731436133384705\n",
      "0.043545786291360855\n",
      "0.043822240084409714\n",
      "0.04328197240829468\n",
      "0.04353666305541992\n",
      "0.042431727051734924\n",
      "0.04279335215687752\n",
      "0.04271789267659187\n",
      "0.04202072322368622\n",
      "0.04191795364022255\n",
      "0.0422329418361187\n",
      "0.0418669693171978\n",
      "0.042157132178545\n",
      "0.0415007509291172\n",
      "0.041582465171813965\n",
      "0.04145282134413719\n",
      "0.04141102358698845\n",
      "0.04184039309620857\n",
      "0.04039163514971733\n",
      "0.04057442769408226\n",
      "0.04088426008820534\n",
      "0.04055230692028999\n",
      "0.04077961668372154\n",
      "0.04026208445429802\n",
      "0.04012263938784599\n",
      "0.040238019078969955\n",
      "0.03973307088017464\n",
      "0.03975972905755043\n",
      "0.03956425562500954\n",
      "0.03935379534959793\n",
      "0.03903695568442345\n",
      "0.03936269134283066\n",
      "0.03957569599151611\n",
      "0.03906248137354851\n",
      "0.038196731358766556\n",
      "0.0387740321457386\n",
      "0.038913410156965256\n",
      "0.038576845079660416\n",
      "0.03811776265501976\n",
      "0.038726504892110825\n",
      "0.03794474899768829\n",
      "0.03823936730623245\n",
      "0.037876784801483154\n",
      "0.03767845034599304\n",
      "0.0370827279984951\n",
      "0.0374704971909523\n",
      "0.03703122213482857\n",
      "0.03748103231191635\n",
      "0.037134405225515366\n",
      "0.036800023168325424\n",
      "0.03650112822651863\n",
      "0.03695780038833618\n",
      "0.03637309744954109\n",
      "0.036302272230386734\n",
      "0.03626037761569023\n",
      "0.03646916151046753\n",
      "0.03607601672410965\n",
      "0.0360725037753582\n",
      "0.03565790131688118\n",
      "0.03583809360861778\n",
      "0.03592166304588318\n",
      "0.03544687479734421\n",
      "0.035205740481615067\n",
      "0.03518501669168472\n",
      "0.03490729629993439\n",
      "0.03496040403842926\n",
      "0.03503233194351196\n",
      "0.034619200974702835\n",
      "0.035016294568777084\n",
      "0.03455539047718048\n",
      "0.0345798023045063\n",
      "0.03402356430888176\n",
      "0.0342412106692791\n",
      "0.03455793485045433\n",
      "0.03364681079983711\n",
      "0.034075893461704254\n",
      "0.03358181565999985\n",
      "0.033382706344127655\n",
      "0.033914510160684586\n",
      "0.03364348039031029\n",
      "0.03386840596795082\n",
      "0.033352263271808624\n",
      "0.03325345367193222\n",
      "0.03278066962957382\n",
      "0.033333804458379745\n",
      "0.03309958428144455\n",
      "0.033206451684236526\n",
      "0.03264285996556282\n",
      "0.03311001881957054\n",
      "0.032538242638111115\n",
      "0.03237015753984451\n",
      "0.03231619670987129\n",
      "0.03198789805173874\n",
      "0.03206535801291466\n",
      "0.03183644637465477\n",
      "0.03156331554055214\n",
      "0.03193145617842674\n",
      "0.03202302008867264\n",
      "0.031361173838377\n",
      "0.031185777857899666\n",
      "0.0316912941634655\n",
      "0.0314386822283268\n",
      "0.03161323443055153\n",
      "0.0313849039375782\n",
      "0.031046167016029358\n",
      "0.031026067212224007\n",
      "0.030850553885102272\n",
      "0.030806658789515495\n",
      "0.030569622293114662\n",
      "0.030693480744957924\n",
      "0.03060331381857395\n",
      "0.030300533398985863\n",
      "0.030452264472842216\n",
      "0.030666640028357506\n",
      "0.030477913096547127\n",
      "0.030215369537472725\n",
      "0.03044114261865616\n",
      "0.030153675004839897\n",
      "0.030148247256875038\n",
      "0.029929451644420624\n",
      "0.029864555224776268\n",
      "0.02940303273499012\n",
      "0.03001033514738083\n",
      "0.029355274513363838\n",
      "0.029668720439076424\n",
      "0.029292061924934387\n",
      "0.029382498934864998\n",
      "0.029043281450867653\n",
      "0.02894597314298153\n",
      "0.028990082442760468\n",
      "0.028743192553520203\n",
      "0.028840703889727592\n",
      "0.028620043769478798\n",
      "0.028884582221508026\n",
      "0.028435342013835907\n",
      "0.028529152274131775\n",
      "0.0285637304186821\n",
      "0.028294861316680908\n",
      "0.028540195897221565\n",
      "0.028391903266310692\n",
      "0.028138622641563416\n",
      "0.028144657611846924\n",
      "0.028080543503165245\n",
      "0.027839859947562218\n",
      "0.02779701165854931\n",
      "0.02787763439118862\n",
      "0.027852527797222137\n",
      "0.027777215465903282\n",
      "0.027546623721718788\n",
      "0.02764636091887951\n",
      "0.02732923813164234\n",
      "0.027461357414722443\n",
      "0.02707519382238388\n",
      "0.02739948034286499\n",
      "0.027020832523703575\n",
      "0.02696285955607891\n",
      "0.02714485675096512\n",
      "0.026808448135852814\n",
      "0.026639431715011597\n",
      "0.026600316166877747\n",
      "0.026384910568594933\n",
      "0.026560073718428612\n",
      "0.02634972520172596\n",
      "0.0266976710408926\n",
      "0.02642354555428028\n",
      "0.026302633807063103\n",
      "0.026325248181819916\n",
      "0.026116011664271355\n",
      "0.025763027369976044\n",
      "0.02585725300014019\n",
      "0.025723477825522423\n",
      "0.02588716708123684\n",
      "0.02588529884815216\n",
      "0.02583094872534275\n",
      "0.025716610252857208\n",
      "0.025874944403767586\n",
      "0.025583207607269287\n",
      "0.02548070251941681\n",
      "0.02552861161530018\n",
      "0.025488009676337242\n",
      "0.025060398504137993\n",
      "0.02533966302871704\n",
      "0.025028042495250702\n",
      "0.02511465735733509\n",
      "0.025219552218914032\n",
      "0.025119207799434662\n",
      "0.024742359295487404\n",
      "0.02529151923954487\n",
      "0.0247177854180336\n",
      "0.024281756952404976\n",
      "0.024492913857102394\n",
      "0.024577828124165535\n",
      "0.024311212822794914\n",
      "0.024611445143818855\n",
      "0.02414262481033802\n",
      "0.024378279224038124\n",
      "0.024298017844557762\n",
      "0.024281522259116173\n",
      "0.024324676021933556\n",
      "0.02380622737109661\n",
      "0.024205869063735008\n",
      "0.023838169872760773\n",
      "0.0239276010543108\n",
      "0.02367863804101944\n",
      "0.023788290098309517\n",
      "0.02369324117898941\n",
      "0.02369106002151966\n",
      "0.023661723360419273\n",
      "0.02361431159079075\n",
      "0.02353263460099697\n",
      "0.02324037440121174\n",
      "0.02353356033563614\n",
      "0.023315802216529846\n",
      "0.02320108376443386\n",
      "0.023387648165225983\n",
      "0.02310401014983654\n",
      "0.023016922175884247\n",
      "0.023129751905798912\n",
      "0.022828392684459686\n",
      "0.02285468392074108\n",
      "0.023080207407474518\n",
      "0.022838721051812172\n",
      "0.02281402237713337\n",
      "0.022680452093482018\n",
      "0.022403694689273834\n",
      "0.022327767685055733\n",
      "0.022489234805107117\n",
      "0.022342130541801453\n",
      "0.022348346188664436\n",
      "0.022296590730547905\n",
      "0.021798640489578247\n",
      "0.022211970761418343\n",
      "0.022637570276856422\n",
      "0.0220969095826149\n",
      "0.021973276510834694\n",
      "0.021838262677192688\n",
      "0.021632811054587364\n",
      "0.021851971745491028\n",
      "0.021690303459763527\n",
      "0.021837299689650536\n",
      "0.021865738555788994\n",
      "0.02172183245420456\n",
      "0.021608224138617516\n",
      "0.02179712802171707\n",
      "0.02148197405040264\n",
      "0.02137001045048237\n",
      "0.02140268124639988\n",
      "0.02149227075278759\n",
      "0.021398337557911873\n",
      "0.02126547507941723\n",
      "0.021165644749999046\n",
      "0.02098160795867443\n",
      "0.021275809034705162\n",
      "0.021267378702759743\n",
      "0.02096603251993656\n",
      "0.020955247804522514\n",
      "0.0211208313703537\n",
      "0.020849430933594704\n",
      "0.020880071446299553\n",
      "0.020772766321897507\n",
      "0.020811861380934715\n",
      "0.020833848044276237\n",
      "0.020756661891937256\n",
      "0.020875701680779457\n",
      "0.020506002008914948\n",
      "0.020364699885249138\n",
      "0.020880335941910744\n",
      "0.020461199805140495\n",
      "0.02043396793305874\n",
      "0.02035353146493435\n",
      "0.020386775955557823\n",
      "0.02002022974193096\n",
      "0.020231787115335464\n",
      "0.020020557567477226\n",
      "0.020127464085817337\n",
      "0.01995805837213993\n",
      "0.01983466185629368\n",
      "0.019853873178362846\n",
      "0.019882449880242348\n",
      "0.019873695448040962\n",
      "0.020020032301545143\n",
      "0.01992066390812397\n",
      "0.019647344946861267\n",
      "0.019710006192326546\n",
      "0.019621282815933228\n",
      "0.019623616710305214\n",
      "0.01953786425292492\n",
      "0.019729694351553917\n",
      "0.019339367747306824\n",
      "0.019529718905687332\n",
      "0.01943223550915718\n",
      "0.01940205879509449\n",
      "0.019260192289948463\n",
      "0.01912897638976574\n",
      "0.01928963139653206\n",
      "0.019108368083834648\n",
      "0.01912112906575203\n",
      "0.01911822520196438\n",
      "0.019123807549476624\n",
      "0.01891091652214527\n",
      "0.01879090815782547\n",
      "0.01891382969915867\n",
      "0.018742764368653297\n",
      "0.01910972408950329\n",
      "0.018675457686185837\n",
      "0.018527161329984665\n",
      "0.018497861921787262\n",
      "0.018649140372872353\n",
      "0.018691422417759895\n",
      "0.018569281324744225\n",
      "0.018615340813994408\n",
      "0.018625788390636444\n",
      "0.018283596262335777\n",
      "0.018442096188664436\n",
      "0.0182651337236166\n",
      "0.01819605939090252\n",
      "0.01810302957892418\n",
      "0.018376324325799942\n",
      "0.018229831010103226\n",
      "0.018205231055617332\n",
      "0.018114088103175163\n",
      "0.01805577613413334\n",
      "0.017973093315958977\n",
      "0.018107693642377853\n",
      "0.01793236844241619\n",
      "0.018078520894050598\n",
      "0.017785169184207916\n",
      "0.017807941883802414\n",
      "0.017725279554724693\n",
      "0.01773138903081417\n",
      "0.017578735947608948\n",
      "0.017738642171025276\n",
      "0.017602743580937386\n",
      "0.017472965642809868\n",
      "0.017418986186385155\n",
      "0.017364049330353737\n",
      "0.01748640276491642\n",
      "0.017406079918146133\n",
      "0.017429018393158913\n",
      "0.017338404431939125\n",
      "0.01742870919406414\n",
      "0.017336687073111534\n",
      "0.017259811982512474\n",
      "0.017055489122867584\n",
      "0.017471065744757652\n",
      "0.017051205039024353\n",
      "0.017187094315886497\n",
      "0.016951048746705055\n",
      "0.017075613141059875\n",
      "0.017074335366487503\n",
      "0.01690160669386387\n",
      "0.01703375019133091\n",
      "0.01682555116713047\n",
      "0.016916170716285706\n",
      "0.016902927309274673\n",
      "0.016838500276207924\n",
      "0.016782822087407112\n",
      "0.016629301011562347\n",
      "0.017016982659697533\n",
      "0.016777200624346733\n",
      "0.016561241820454597\n",
      "0.016686536371707916\n",
      "0.016535909846425056\n",
      "0.0166386179625988\n",
      "0.01647667959332466\n",
      "0.01646413840353489\n",
      "0.016429999843239784\n",
      "0.01648659072816372\n",
      "0.016325708478689194\n",
      "0.01646118052303791\n",
      "0.016267502680420876\n",
      "0.01600506156682968\n",
      "0.016298912465572357\n",
      "0.016248974949121475\n",
      "0.01606515422463417\n",
      "0.016232693567872047\n",
      "0.01607639156281948\n",
      "0.01609821431338787\n",
      "0.016219178214669228\n",
      "0.01598222926259041\n",
      "0.015987029299139977\n",
      "0.015930749475955963\n",
      "0.01593090035021305\n",
      "0.01596003770828247\n",
      "0.015780476853251457\n",
      "0.015833500772714615\n",
      "0.01586117222905159\n",
      "0.015773165971040726\n",
      "0.01577993482351303\n",
      "0.015693051740527153\n",
      "0.015723703429102898\n",
      "0.015810998156666756\n",
      "0.015601723454892635\n",
      "0.015488090924918652\n",
      "0.01536368578672409\n",
      "0.015631087124347687\n",
      "0.01563888043165207\n",
      "0.015412595123052597\n",
      "[step 1] loss=0.0154  \n",
      "0.015608381479978561\n",
      "0.01535869762301445\n",
      "0.015540801919996738\n",
      "0.01537538692355156\n",
      "0.015352562069892883\n",
      "0.014992814511060715\n",
      "0.015243440866470337\n",
      "0.015243224799633026\n",
      "0.01510819885879755\n",
      "0.015054091811180115\n",
      "0.015168838202953339\n",
      "0.015135306864976883\n",
      "0.015044142492115498\n",
      "0.015170440077781677\n",
      "0.014809838496148586\n",
      "0.014943080954253674\n",
      "0.014904424548149109\n",
      "0.014840695075690746\n",
      "0.01482656691223383\n",
      "0.014950008131563663\n",
      "0.014838703908026218\n",
      "0.01495746523141861\n",
      "0.014641002751886845\n",
      "0.014677372761070728\n",
      "0.014737543649971485\n",
      "0.014779833145439625\n",
      "0.014673733152449131\n",
      "0.014653820544481277\n",
      "0.01483315508812666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m loss = F.cross_entropy(y_hat, batch)\n\u001b[32m     17\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     22\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = ToolModel(vocab_size=501, d_model=D_MODEL, n_heads=N_HEADS, depth=DEPTH)\n",
    "model.to(torch.device(\"cuda\"))\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    for step, batch in enumerate(loader, start=1):\n",
    "        batch = batch.to(next(model.parameters()).device)  # ensure same device\n",
    "\n",
    "        y_hat = model(batch)\n",
    "        y_hat = y_hat.transpose(2,1)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, batch)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"[step {epoch}] \"\n",
    "            f\"loss={loss.item():.4f}  \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f3c5324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba1ff4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 101])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03465b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
