{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79046285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea75952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, max_seq_len=4096, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.d_head, 2).float() / self.d_head)).to(self.device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self.cos, self.sin = self._build_freqs()\n",
    "\n",
    "    def _build_freqs(self):\n",
    "        t = torch.arange(self.max_seq_len, device=self.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()[:, None, None, :]\n",
    "        sin = emb.sin()[:, None, None, :]\n",
    "        cos = cos.reshape(1, self.max_seq_len, 1, self.d_head)\n",
    "        sin = sin.reshape(1, self.max_seq_len, 1, self.d_head)\n",
    "        return cos, sin\n",
    "\n",
    "    def _rotate_half(self, x):\n",
    "        x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def _apply_rotary_pos_emb(self, q, k):\n",
    "        B, S, H, D = q.shape\n",
    "        if q.shape != k.shape:\n",
    "            raise NotImplementedError(\"q and k must have the same shape\")\n",
    "        cos, sin = self.cos[:, :S, :, :], self.sin[:, :S, :, :]\n",
    "        return (q * cos) + (self._rotate_half(q) * sin), (k * cos) + (self._rotate_half(k) * sin)\n",
    "\n",
    "    def __call__(self, q, k):\n",
    "        return self._apply_rotary_pos_emb(q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_experts, dropout=0.0, top_k=1):\n",
    "        super().__init__()\n",
    "        if top_k != 1:\n",
    "            raise NotImplementedError(\"Top-k is not implemented for MoE\")\n",
    "        self.top_k = top_k\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.experts = nn.ModuleList([nn.Linear(in_dim, out_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x, expert_probs, inference=False):\n",
    "        if inference:\n",
    "            raise NotImplementedError(\"Inference is not implemented for MoE\")\n",
    "\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        if expert_probs.shape != (B, self.num_experts):\n",
    "            raise ValueError(f\"Expert probabilities must be of shape (B, num_experts), got {expert_probs.shape}\")\n",
    "        \n",
    "        expert_idx = torch.argmax(expert_probs, dim=-1)\n",
    "\n",
    "        x_out = torch.zeros(B, S, self.out_dim, device=x.device)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            x_out[expert_idx == i, : , :] += expert(x[expert_idx == i, : , :])\n",
    "\n",
    "        return x_out, expert_idx\n",
    "\n",
    "\n",
    "    def __call__(self, x, expert_probs, inference=False):\n",
    "        return self.forward(x, expert_probs, inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, dropout=0.0, is_causal=True, use_moe=False, num_experts=4, router_idx=None, verbose_router=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_experts = num_experts\n",
    "        self.dropout = dropout\n",
    "        self.router_idx = router_idx\n",
    "        self.is_causal = is_causal\n",
    "        self.verbose_router = verbose_router\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.rope = RoPE(d_model=dim, n_heads=n_heads, device=torch.device(\"cuda\"), max_seq_len=4096)\n",
    "\n",
    "        self.use_moe = use_moe\n",
    "        if use_moe:\n",
    "            if router_idx is None:\n",
    "                raise ValueError(\"router_idx must be provided when using MoE\")\n",
    "            self.router = nn.Linear(dim, num_experts)\n",
    "            self.qkv = MoE(dim, 3 * dim, num_experts, dropout=dropout)\n",
    "            self.o = MoE(dim, dim, num_experts, dropout=dropout)\n",
    "            self.mlp_in = MoE(dim, int(dim * mlp_ratio), num_experts, dropout=dropout)\n",
    "            self.mlp_out = MoE(int(dim * mlp_ratio), dim, num_experts, dropout=dropout)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, 3 * dim)\n",
    "            self.o = nn.Linear(dim, dim)\n",
    "            self.mlp_in = nn.Linear(dim, int(dim * mlp_ratio))\n",
    "            self.mlp_out = nn.Linear(int(dim * mlp_ratio), dim)\n",
    "\n",
    "    def mlp(self, x, expert_probs=None):\n",
    "        if self.use_moe:\n",
    "            x, expert_idx = self.mlp_in(x, expert_probs)\n",
    "            x = F.gelu(x)\n",
    "            x, _ = self.mlp_out(x, expert_probs)\n",
    "        else:\n",
    "            x = F.gelu(self.mlp_in(x))\n",
    "            x = self.mlp_out(x)\n",
    "        return x\n",
    "\n",
    "    def attn(self, x):\n",
    "        B, S, D = x.shape\n",
    "        head_dim = D // self.n_heads\n",
    "        num_heads = self.n_heads\n",
    "\n",
    "        if self.use_moe:\n",
    "            router_out = self.router(x[:, self.router_idx])\n",
    "            expert_probs = F.softmax(router_out, dim=-1)\n",
    "            if self.verbose_router:\n",
    "                top_experts = torch.argmax(expert_probs, dim=-1)  # [batch, tokens]\n",
    "                num_experts = expert_probs.size(-1)\n",
    "                counts = torch.bincount(top_experts.flatten(), minlength=num_experts)\n",
    "                usage = counts.float() / counts.sum() * 100\n",
    "                print(f\"Expert usage (%): {usage.tolist()}\")\n",
    "\n",
    "            qkv, expert_idx = self.qkv(x, expert_probs)\n",
    "        else:\n",
    "            qkv = self.qkv(x)\n",
    "            expert_idx = None\n",
    "            expert_probs = None\n",
    "\n",
    "        q, k, v = qkv.split(D, dim=2)\n",
    "        q = q.view(B, S, num_heads, head_dim)\n",
    "        k = k.view(B, S, num_heads, head_dim)\n",
    "        v = v.view(B, S, num_heads, head_dim)\n",
    "\n",
    "        q, k = self.rope(q, k)\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=self.is_causal)\n",
    "\n",
    "        attn_out = attn_out.permute(0, 2, 1, 3)\n",
    "        attn_out = attn_out.view(B, S, D)\n",
    "        if self.use_moe:\n",
    "            attn_out, expert_idx = self.o(attn_out, expert_probs)\n",
    "        else:\n",
    "            attn_out = self.o(attn_out)\n",
    "            expert_idx = None\n",
    "\n",
    "        return attn_out, expert_idx, expert_probs\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out, _, expert_probs = self.attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        if self.use_moe:\n",
    "            mlp_out = self.mlp(x, expert_probs)\n",
    "        else:\n",
    "            mlp_out = self.mlp(x)\n",
    "\n",
    "        x = self.norm2(x + mlp_out)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ae119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/addition.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_x = torch.tensor(df['lhs_seq'].apply(ast.literal_eval).tolist())\n",
    "_y = torch.tensor(df['rhs_seq'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "X = torch.cat([_x, _y], dim=1).to(device)\n",
    "\n",
    "loss_mask = torch.ones_like(X).to(device)\n",
    "loss_mask[:, _x.shape[1]:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a43cdfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"0123456789ri+=_\"\n",
    "vocab_size = len(vocab)\n",
    "def encode(s):\n",
    "    return [vocab.index(c) for c in s]\n",
    "def decode(l):\n",
    "    return ''.join([vocab[i] for i in l])\n",
    "    \n",
    "IGNORE_INDEX = vocab_size-1\n",
    "D_MODEL = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a61c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, X, loss_mask):\n",
    "        self.X = X\n",
    "        self.loss_mask = loss_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.loss_mask[idx]\n",
    "\n",
    "dataset = AdditionDataset(X, loss_mask)\n",
    "loader = DataLoader(dataset, batch_size=8000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdditionModel(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, n_layers, vocab_size, num_experts):\n",
    "#         super().__init__()\n",
    "#         self.encoder = nn.Embedding(vocab_size, d_model)\n",
    "#         self.transformer = nn.ModuleList([TransformerBlock(d_model, n_heads, is_causal=True, use_moe=False) for _ in range(n_layers)])\n",
    "#         self.decoder = nn.Linear(d_model, vocab_size)\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.moe = TransformerBlock(d_model, n_heads, is_causal=True, use_moe=True, num_experts=num_experts, router_idx=7)\n",
    "#         # self.router = nn.Linear(d_model, num_experts)\n",
    "#         self.to(self.device)\n",
    "\n",
    "\n",
    "#     def preprocess(self, x):\n",
    "#         self.moe(x)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.transformer:\n",
    "#             x = layer(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "\n",
    "#     def train(self, optimizer,epochs=1000):\n",
    "#         for epoch in (range(epochs)):\n",
    "#             for i, (batch_x, batch_mask) in enumerate(loader):\n",
    "#                 x = self.encoder(batch_x)\n",
    "#                 # x[:, :8, :] = self.preprocess(x[:, :8,:])\n",
    "\n",
    "#                 y_hat = self.forward(x[:, :-1])\n",
    "#                 loss = F.cross_entropy(\n",
    "#                     input = y_hat.permute(0, 2, 1),\n",
    "#                     target= batch_x[:, 1:],\n",
    "#                     reduction=\"none\"\n",
    "#                 )\n",
    "#                 masked_loss = loss * batch_mask[:, 1:]\n",
    "#                 loss = masked_loss.mean()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 print(f\"Epoch {i} loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AdditionModel(D_MODEL, N_HEADS, N_LAYERS, vocab_size, 4)\n",
    "# optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "# model.train(optimizer, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRM(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, max_recursions, vocab_size, num_experts, recursive_idx=recursive_idx, router_idx=6):\n",
    "        super().__init__()\n",
    "        self.max_recursions = max_recursions\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = TransformerBlock(d_model, n_heads, is_causal=True, use_moe=True, num_experts=num_experts, router_idx=router_idx, verbose_router=True)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _ in range(self.max_recursions):\n",
    "            x = self.transformer(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, optimizer,epochs=1):\n",
    "        for _ in (range(epochs)):\n",
    "            for i, (batch_x, batch_mask) in enumerate(loader):\n",
    "                x = self.encoder(batch_x)\n",
    "\n",
    "                y_hat = self.forward(x[:, :-1])\n",
    "                loss = F.cross_entropy(\n",
    "                    input = y_hat.permute(0, 2, 1),\n",
    "                    target= batch_x[:, 1:],\n",
    "                    reduction=\"none\"\n",
    "                )\n",
    "                masked_loss = loss * batch_mask[:, 1:]\n",
    "                loss = masked_loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                print(f\"Epoch {i} loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ea185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RRM(64, N_HEADS, max_recursions=4, vocab_size=vocab_size, num_experts=4)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "model.train(optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4f782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
