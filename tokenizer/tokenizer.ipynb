{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91fc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd37bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "import re\n",
    "from typing import List, Tuple, Union\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1acb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_CHARS = set(\"0123456789+-*/=() \")\n",
    "MIN_INT = -500\n",
    "MAX_INT = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047e3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_math_char(ch: str) -> bool:\n",
    "    # allowed characters INSIDE math spans\n",
    "    return ch.isdigit() or ch in \"+-*/=() \"\n",
    "\n",
    "\n",
    "\n",
    "def split_math_text_spans(text: str) -> List[Tuple[bool, str]]:\n",
    "    spans = []\n",
    "    if not text:\n",
    "        return spans\n",
    "\n",
    "    # First pass – split by char-class transitions\n",
    "    raw_spans = []\n",
    "    current = [text[0]]\n",
    "    current_is_math = is_math_char(text[0])\n",
    "\n",
    "    for ch in text[1:]:\n",
    "        ch_is_math = is_math_char(ch)\n",
    "        if ch_is_math == current_is_math:\n",
    "            current.append(ch)\n",
    "        else:\n",
    "            raw_spans.append((current_is_math, \"\".join(current)))\n",
    "            current = [ch]\n",
    "            current_is_math = ch_is_math\n",
    "\n",
    "    raw_spans.append((current_is_math, \"\".join(current)))\n",
    "\n",
    "    # Second pass – validate math spans (MUST contain a digit)\n",
    "    final_spans = []\n",
    "    for is_math, span in raw_spans:\n",
    "        if is_math:\n",
    "            # check if span actually contains math (digit required)\n",
    "            if any(c.isdigit() for c in span):\n",
    "                final_spans.append((True, span))\n",
    "            else:\n",
    "                # treat span as text if no digits\n",
    "                final_spans.append((False, span))\n",
    "        else:\n",
    "            final_spans.append((False, span))\n",
    "\n",
    "    return final_spans\n",
    "\n",
    "\n",
    "MATH_TOKEN_RE = re.compile(r\"\"\"\n",
    "    \\s+              |   # whitespace (skip)\n",
    "    (-?\\d+)          |   # integer, possibly negative\n",
    "    ([+\\-*/=()])         # operators / parens\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def tokenize_math_expr(expr: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Turn a math expression string like '-47 * -2 - 35 * -19 = 759'\n",
    "    into ['-47', '*', '-2', '-', '35', '*', '-19', '=', '759'].\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    n = len(expr)\n",
    "    if \"--\" in expr:\n",
    "        raise ValueError(\"Invalid unary sequence '--'\")\n",
    "\n",
    "    while pos < n:\n",
    "        m = MATH_TOKEN_RE.match(expr, pos)\n",
    "        if not m:\n",
    "            # Anything non-math that slipped in: you can choose to raise or\n",
    "            # fall back to char-level; for now, be strict.\n",
    "            raise ValueError(f\"Unexpected character in math expr: {expr[pos]!r} at position {pos}\")\n",
    "        if m.group(1):  # integer token\n",
    "            val = int(m.group(1))\n",
    "            if val < MIN_INT or val > MAX_INT:\n",
    "                # Not a valid integer token -> treat whole span as TEXT instead\n",
    "                raise ValueError(\"Integer out of allowed range\")\n",
    "            tokens.append(m.group(1))\n",
    "\n",
    "        elif m.group(2):  # operator/parens\n",
    "            tokens.append(m.group(2))\n",
    "        # group(0) is full match; could be whitespace, which we ignore\n",
    "        pos = m.end()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class HybridMathTokenizer:\n",
    "    \"\"\"\n",
    "    Hybrid tokenizer:\n",
    "      - Uses a base HF tokenizer for natural language.\n",
    "      - Uses custom integer/operator tokens for math spans.\n",
    "\n",
    "    Math spans are detected heuristically via MATH_CHARS.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_tokenizer: Union[str, \"PreTrainedTokenizerBase\"],\n",
    "        min_int: int = -500,\n",
    "        max_int: int = 500,\n",
    "        add_expr_markers: bool = False,\n",
    "        extra_special_tokens: List[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        base_tokenizer: HF tokenizer name or instance (e.g. \"meta-llama/Llama-3-8B-Instruct\").\n",
    "        min_int, max_int: inclusive integer range for atomic number tokens.\n",
    "        add_expr_markers: if True, adds <EXPR_START> and <EXPR_END> tokens around math spans.\n",
    "        extra_special_tokens: optional list of extra tokens (e.g. <FACT_1>, <READY_1>).\n",
    "        \"\"\"\n",
    "        if isinstance(base_tokenizer, str):\n",
    "            if AutoTokenizer is None:\n",
    "                raise ImportError(\"transformers is required to load a tokenizer by name\")\n",
    "            self.base = AutoTokenizer.from_pretrained(base_tokenizer, use_fast=True)\n",
    "        else:\n",
    "            self.base = base_tokenizer\n",
    "\n",
    "        self.min_int = min_int\n",
    "        self.max_int = max_int\n",
    "        self.add_expr_markers = add_expr_markers\n",
    "\n",
    "        # Build list of tokens to add to the base tokenizer\n",
    "        new_tokens = []\n",
    "\n",
    "        # Operators / parens\n",
    "        self.op_tokens = [\"+\", \"-\", \"*\", \"/\", \"=\", \"(\", \")\"]\n",
    "        new_tokens.extend(self.op_tokens)\n",
    "\n",
    "        # Integers as atomic tokens\n",
    "        self.int_tokens = [str(i) for i in range(min_int, max_int + 1)]\n",
    "        new_tokens.extend(self.int_tokens)\n",
    "\n",
    "        # Optional expression markers and extra specials\n",
    "        self.expr_start = \"<EXPR_START>\"\n",
    "        self.expr_end = \"<EXPR_END>\"\n",
    "        self.special_extra = extra_special_tokens or []\n",
    "\n",
    "        if add_expr_markers:\n",
    "            new_tokens.extend([self.expr_start, self.expr_end])\n",
    "\n",
    "        new_tokens.extend(self.special_extra)\n",
    "\n",
    "        # Add to base tokenizer vocab\n",
    "        # NOTE: add_tokens will only add tokens that aren't already in vocab\n",
    "        self.base.add_tokens(new_tokens)\n",
    "\n",
    "        # For convenience, keep the token->id mapping\n",
    "        self._update_maps()\n",
    "\n",
    "    def _update_maps(self):\n",
    "        self.token_to_id = self.base.get_vocab()\n",
    "        # Reverse map\n",
    "        self.id_to_token = {i: t for t, i in self.token_to_id.items()}\n",
    "\n",
    "    # ------------- public API -------------\n",
    "\n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text into a single list of token ids, with math spans\n",
    "        tokenized using atomic integer/operator tokens.\n",
    "        \"\"\"\n",
    "        spans = split_math_text_spans(text)\n",
    "        all_ids: List[int] = []\n",
    "\n",
    "        for is_math, span in spans:\n",
    "            if not span:\n",
    "                continue\n",
    "\n",
    "            if is_math:\n",
    "                # math span: tokenize via our math rules\n",
    "                try:\n",
    "                    math_tokens = tokenize_math_expr(span)\n",
    "                except ValueError:\n",
    "                    # fallback: this isn't valid math for our domain → treat as text\n",
    "                    ids = self.base.encode(span, add_special_tokens=False)\n",
    "                    all_ids.extend(ids)\n",
    "                    continue\n",
    "\n",
    "                if self.add_expr_markers:\n",
    "                    math_tokens = [self.expr_start] + math_tokens + [self.expr_end]\n",
    "                ids = self.base.convert_tokens_to_ids(math_tokens)\n",
    "                all_ids.extend(ids)\n",
    "            else:\n",
    "                # normal text span: delegate to base tokenizer\n",
    "                ids = self.base.encode(\n",
    "                    span,\n",
    "                    add_special_tokens=False,  # we handle global special tokens outside\n",
    "                )\n",
    "                all_ids.extend(ids)\n",
    "\n",
    "        if add_special_tokens and hasattr(self.base, \"bos_token_id\") and hasattr(self.base, \"eos_token_id\"):\n",
    "            # Very simple: wrap with BOS/EOS if they exist\n",
    "            bos = [] if self.base.bos_token_id is None else [self.base.bos_token_id]\n",
    "            eos = [] if self.base.eos_token_id is None else [self.base.eos_token_id]\n",
    "            return bos + all_ids + eos\n",
    "\n",
    "        return all_ids\n",
    "\n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Decode token ids back to text via the base tokenizer.\n",
    "        For math tokens, since we used actual string forms as tokens,\n",
    "        this round-trips cleanly.\n",
    "        \"\"\"\n",
    "        return self.base.decode(ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    # Convenience helpers if you want to bypass span splitting:\n",
    "\n",
    "    def encode_math_only(self, expr: str) -> List[int]:\n",
    "        \"\"\"Encode a pure math expression string.\"\"\"\n",
    "        tokens = tokenize_math_expr(expr)\n",
    "        if self.add_expr_markers:\n",
    "            tokens = [self.expr_start] + tokens + [self.expr_end]\n",
    "        return self.base.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    def encode_text_only(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode pure natural language through the base tokenizer.\"\"\"\n",
    "        return self.base.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea486b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2ff6ee6d5a4e54ae390ef57622f653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--allenai--OLMoE-1B-7B-0125-SFT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b6d286d7c54d5a976a969e399f49d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bf857b555143d78a6138f61c03b016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50279, 50780, 50733, 11, 50778, 30, 2787, 50781, 50279]\n",
      "<EXPR_START>-47*-2=79<EXPR_END>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0125-SFT\", use_fast=True)\n",
    "\n",
    "tok = HybridMathTokenizer(base_tokenizer=base, min_int=MIN_INT, max_int=MAX_INT, add_expr_markers=True)\n",
    "\n",
    "s = \"-47 * -2 = 79\"\n",
    "ids = tok.encode(s)\n",
    "print(ids)\n",
    "print(tok.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d2cb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Compare the sizes: $\\pi$ ____ $3.14$ (fill in the blank with $=$, $>$, or $<$).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24a91c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50279, 33925, 209, 783, 209, 84, 4219, 27, 209, 1202, 2059, 5, 209, 1713, 209, 5, 50780, 20, 50781, 15, 50780, 1047, 50781, 5, 209, 9, 9337, 209, 249, 209, 783, 209, 22473, 209, 3113, 209, 5, 30, 1366, 209, 5, 31, 1366, 209, 263, 209, 5, 29, 5, 10, 15, 50279]\n",
      "Compare the sizes: $\\pi$ ____ $<EXPR_START>3<EXPR_END>.<EXPR_START>14<EXPR_END>$ (fill in the blank with $=$, $>$, or $<$).\n"
     ]
    }
   ],
   "source": [
    "ids = tok.encode(s)\n",
    "print(ids)\n",
    "print(tok.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36023667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode([151643])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1581ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "INPUT: -47 * -2 = 94\n",
      "IDS: [152656, 152118, 9, 152163, 28, 152249, 152657]\n",
      "DECODED: <EXPR_START>-47*-2=94<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 0 + 0 = 0\n",
      "IDS: [152656, 15, 10, 15, 28, 15, 152657]\n",
      "DECODED: <EXPR_START>0+0=0<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 12 - 5 + 3 - 2\n",
      "IDS: [152656, 152167, 12, 20, 10, 18, 12, 17, 152657]\n",
      "DECODED: <EXPR_START>12-5+3-2<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 999 + -999\n",
      "IDS: [152254, 24, 220, 10, 220, 152066, 24]\n",
      "DECODED: 999 + -999\n",
      "\n",
      "====================\n",
      "INPUT: (-3) * (-4)\n",
      "IDS: [152656, 7, 152162, 8, 9, 7, 152161, 8, 152657]\n",
      "DECODED: <EXPR_START>(-3)*(-4)<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: Compute -47 * -2 = 94 please.\n",
      "IDS: [46254, 152656, 152118, 9, 152163, 28, 152249, 152657, 30021, 13]\n",
      "DECODED: Compute<EXPR_START>-47*-2=94<EXPR_END>please.\n",
      "\n",
      "====================\n",
      "INPUT: What is (12 * 4) - 3?\n",
      "IDS: [3838, 220, 285, 152656, 7, 152167, 9, 19, 8, 12, 18, 152657, 30]\n",
      "DECODED: What is<EXPR_START>(12*4)-3<EXPR_END>?\n",
      "\n",
      "====================\n",
      "INPUT: -47*-2=94 is correct.\n",
      "IDS: [152656, 152118, 9, 152163, 28, 152249, 152657, 285, 220, 19928, 13]\n",
      "DECODED: <EXPR_START>-47*-2=94<EXPR_END>is correct.\n",
      "\n",
      "====================\n",
      "INPUT: Check: 3+4=7. Good?\n",
      "IDS: [3973, 25, 152656, 18, 10, 19, 28, 22, 152657, 13, 220, 15216, 30]\n",
      "DECODED: Check:<EXPR_START>3+4=7<EXPR_END>. Good?\n",
      "\n",
      "====================\n",
      "INPUT: -47 * -2 = 94.\n",
      "IDS: [152656, 152118, 9, 152163, 28, 152249, 152657, 13]\n",
      "DECODED: <EXPR_START>-47*-2=94<EXPR_END>.\n",
      "\n",
      "====================\n",
      "INPUT: (-3) * (-4), obviously.\n",
      "IDS: [152656, 7, 152162, 8, 9, 7, 152161, 8, 152657, 11, 220, 674, 18281, 13]\n",
      "DECODED: <EXPR_START>(-3)*(-4)<EXPR_END>, obviously.\n",
      "\n",
      "====================\n",
      "INPUT: What?\n",
      "IDS: [3838, 30]\n",
      "DECODED: What?\n",
      "\n",
      "====================\n",
      "INPUT: test...\n",
      "IDS: [1944, 1112]\n",
      "DECODED: test...\n",
      "\n",
      "====================\n",
      "INPUT: 12? no\n",
      "IDS: [152656, 152167, 152657, 30, 220, 2152]\n",
      "DECODED: <EXPR_START>12<EXPR_END>? no\n",
      "\n",
      "====================\n",
      "INPUT: 12.5\n",
      "IDS: [152656, 152167, 152657, 13, 152656, 20, 152657]\n",
      "DECODED: <EXPR_START>12<EXPR_END>.<EXPR_START>5<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 3.14159\n",
      "IDS: [152656, 18, 152657, 13, 152296, 152214]\n",
      "DECODED: <EXPR_START>3<EXPR_END>.14159\n",
      "\n",
      "====================\n",
      "INPUT: 0.0\n",
      "IDS: [152656, 15, 152657, 13, 152656, 15, 152657]\n",
      "DECODED: <EXPR_START>0<EXPR_END>.<EXPR_START>0<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: -.5\n",
      "IDS: [12, 13, 152656, 20, 152657]\n",
      "DECODED: -.<EXPR_START>5<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 5.\n",
      "IDS: [152656, 20, 152657, 13]\n",
      "DECODED: <EXPR_START>5<EXPR_END>.\n",
      "\n",
      "====================\n",
      "INPUT: 12..5\n",
      "IDS: [152656, 152167, 152657, 496, 152656, 20, 152657]\n",
      "DECODED: <EXPR_START>12<EXPR_END>..<EXPR_START>5<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: --47\n",
      "IDS: [12, 152118]\n",
      "DECODED: --47\n",
      "\n",
      "====================\n",
      "INPUT: 3*-*-2\n",
      "IDS: [152656, 18, 9, 12, 9, 152163, 152657]\n",
      "DECODED: <EXPR_START>3*-*-2<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: abc123\n",
      "IDS: [13683, 152656, 152278, 152657]\n",
      "DECODED: abc<EXPR_START>123<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: 123abc\n",
      "IDS: [152656, 152278, 152657, 13683]\n",
      "DECODED: <EXPR_START>123<EXPR_END>abc\n",
      "\n",
      "====================\n",
      "INPUT:   -47   *   -2    = 94    \n",
      "IDS: [152656, 152118, 9, 152163, 28, 152249, 152657]\n",
      "DECODED: <EXPR_START>-47*-2=94<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: (-3)*(-4)\n",
      "IDS: [152656, 7, 152162, 8, 9, 7, 152161, 8, 152657]\n",
      "DECODED: <EXPR_START>(-3)*(-4)<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT:    (  3 + 4 )  \n",
      "IDS: [152656, 7, 18, 10, 19, 8, 152657]\n",
      "DECODED: <EXPR_START>(3+4)<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: The first: -47 * -2 = 94, the second: 12 * 12 = 144.\n",
      "IDS: [785, 220, 3896, 25, 152656, 152118, 9, 152163, 28, 152249, 152657, 11, 220, 1782, 220, 5569, 25, 152656, 152167, 9, 152167, 28, 152299, 152657, 13]\n",
      "DECODED: The first:<EXPR_START>-47*-2=94<EXPR_END>, the second:<EXPR_START>12*12=144<EXPR_END>.\n",
      "\n",
      "====================\n",
      "INPUT: Compute 3 + 4 = 7 and 8 + 9 = 17.\n",
      "IDS: [46254, 152656, 18, 10, 19, 28, 22, 152657, 437, 152656, 23, 10, 24, 28, 152172, 152657, 13]\n",
      "DECODED: Compute<EXPR_START>3+4=7<EXPR_END>and<EXPR_START>8+9=17<EXPR_END>.\n",
      "\n",
      "====================\n",
      "INPUT: Edge case: 3-4= -1. Weird!\n",
      "IDS: [11656, 220, 5638, 25, 152656, 18, 152161, 28, 152164, 152657, 13, 220, 1654, 2603, 0]\n",
      "DECODED: Edge case:<EXPR_START>3-4=-1<EXPR_END>. Weird!\n",
      "\n",
      "====================\n",
      "INPUT: (((3)))\n",
      "IDS: [152656, 7, 7, 7, 18, 8, 8, 8, 152657]\n",
      "DECODED: <EXPR_START>(((3)))<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: ((3+4)*((2-1)))\n",
      "IDS: [152656, 7, 7, 18, 10, 19, 8, 9, 7, 7, 17, 152164, 8, 8, 8, 152657]\n",
      "DECODED: <EXPR_START>((3+4)*((2-1)))<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: (3*(4-(5+6)))\n",
      "IDS: [152656, 7, 18, 9, 7, 19, 12, 7, 20, 10, 21, 8, 8, 8, 152657]\n",
      "DECODED: <EXPR_START>(3*(4-(5+6)))<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: e=mc^2\n",
      "IDS: [68, 28, 12887, 61, 152656, 17, 152657]\n",
      "DECODED: e=mc^<EXPR_START>2<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: version1.2.3\n",
      "IDS: [4366, 152656, 16, 152657, 13, 152656, 17, 152657, 13, 152656, 18, 152657]\n",
      "DECODED: version<EXPR_START>1<EXPR_END>.<EXPR_START>2<EXPR_END>.<EXPR_START>3<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: file_name-47\n",
      "IDS: [1192, 1269, 152656, 152118, 152657]\n",
      "DECODED: file_name<EXPR_START>-47<EXPR_END>\n",
      "\n",
      "====================\n",
      "INPUT: token-ids=3\n",
      "IDS: [5839, 12, 3365, 152656, 28, 18, 152657]\n",
      "DECODED: token-ids<EXPR_START>=3<EXPR_END>\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    # A. Pure integer arithmetic\n",
    "    \"-47 * -2 = 94\",\n",
    "    \"0 + 0 = 0\",\n",
    "    \"12 - 5 + 3 - 2\",\n",
    "    \"999 + -999\",\n",
    "    \"(-3) * (-4)\",\n",
    "\n",
    "    # B. Math glued to text\n",
    "    \"Compute -47 * -2 = 94 please.\",\n",
    "    \"What is (12 * 4) - 3?\",\n",
    "    \"-47*-2=94 is correct.\",\n",
    "    \"Check: 3+4=7. Good?\",\n",
    "\n",
    "    # C. Stray punctuation\n",
    "    \"-47 * -2 = 94.\",\n",
    "    \"(-3) * (-4), obviously.\",\n",
    "    \"What?\",\n",
    "    \"test...\",\n",
    "    \"12? no\",\n",
    "\n",
    "    # D. Decimals (should NOT be math)\n",
    "    \"12.5\",\n",
    "    \"3.14159\",\n",
    "    \"0.0\",\n",
    "    \"-.5\",\n",
    "    \"5.\",\n",
    "\n",
    "    # E. Garbage mixed\n",
    "    \"12..5\",\n",
    "    \"--47\",\n",
    "    \"3*-*-2\",\n",
    "    \"abc123\",\n",
    "    \"123abc\",\n",
    "\n",
    "    # F. Odd whitespace\n",
    "    \"  -47   *   -2    = 94    \",\n",
    "    \"(-3)*(-4)\",\n",
    "    \"   (  3 + 4 )  \",\n",
    "\n",
    "    # G. Multiple expressions\n",
    "    \"The first: -47 * -2 = 94, the second: 12 * 12 = 144.\",\n",
    "    \"Compute 3 + 4 = 7 and 8 + 9 = 17.\",\n",
    "    \"Edge case: 3-4= -1. Weird!\",\n",
    "\n",
    "    # H. Parentheses hell\n",
    "    \"(((3)))\",\n",
    "    \"((3+4)*((2-1)))\",\n",
    "    \"(3*(4-(5+6)))\",\n",
    "\n",
    "    # I. False alarms (NOT math)\n",
    "    \"e=mc^2\",\n",
    "    \"version1.2.3\",\n",
    "    \"file_name-47\",\n",
    "    \"token-ids=3\",\n",
    "]\n",
    "\n",
    "for s in tests:\n",
    "    print(\"\\n====================\")\n",
    "    print(\"INPUT:\", s)\n",
    "\n",
    "    try:\n",
    "        ids = tok.encode(s, add_special_tokens=False)\n",
    "        print(\"IDS:\", ids)\n",
    "        decoded = tok.decode(ids)\n",
    "        print(\"DECODED:\", decoded)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a88adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
